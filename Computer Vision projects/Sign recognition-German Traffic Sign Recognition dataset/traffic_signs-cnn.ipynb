{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "IMG_WIDTH = 30\n",
    "IMG_HEIGHT = 30\n",
    "NUM_CATEGORIES = 43\n",
    "TEST_SIZE = 0.4\n",
    "filename = \"traffic.h5\"\n",
    "data_dir = r\"traffic_signs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "   # loading data and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # joining data_dir path and folder paths\n",
    "    for dir in os.listdir(data_dir):\n",
    "        folder = os.path.join(data_dir, dir)\n",
    "        \n",
    "        # message\n",
    "        if os.path.isdir(folder):\n",
    "            print(f\"Loading files from {folder}\")\n",
    "        \n",
    "        # looking for images in folders\n",
    "        for file in os.listdir(folder):\n",
    "            # reading and resizing images\n",
    "            img = cv.imread(os.path.join(folder, file))\n",
    "            img = cv.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "            \n",
    "            # appending lists\n",
    "            images.append(img)\n",
    "            labels.append(int(dir))\n",
    "            \n",
    "    # returning lists        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # defining model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        # adding convolutional layer with 32 filters by 3x3 kernal\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation=\"sigmoid\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
    "        \n",
    "        # adding max_pooling layer with pool size 2x2\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # flattening all the units with dropout with 30%\n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        # adding hidden layer with 128 units with 50% dropout\n",
    "        tf.keras.layers.Dense(256, activation=\"sigmoid\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        # adding output layer with 10 units to get probablity for 10 digits with softmax activation fn\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    # compiling model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # getting model summary\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from traffic_signs\\0\n",
      "Loading files from traffic_signs\\1\n",
      "Loading files from traffic_signs\\10\n",
      "Loading files from traffic_signs\\11\n",
      "Loading files from traffic_signs\\12\n",
      "Loading files from traffic_signs\\13\n",
      "Loading files from traffic_signs\\14\n",
      "Loading files from traffic_signs\\15\n",
      "Loading files from traffic_signs\\16\n",
      "Loading files from traffic_signs\\17\n",
      "Loading files from traffic_signs\\18\n",
      "Loading files from traffic_signs\\19\n",
      "Loading files from traffic_signs\\2\n",
      "Loading files from traffic_signs\\20\n",
      "Loading files from traffic_signs\\21\n",
      "Loading files from traffic_signs\\22\n",
      "Loading files from traffic_signs\\23\n",
      "Loading files from traffic_signs\\24\n",
      "Loading files from traffic_signs\\25\n",
      "Loading files from traffic_signs\\26\n",
      "Loading files from traffic_signs\\27\n",
      "Loading files from traffic_signs\\28\n",
      "Loading files from traffic_signs\\29\n",
      "Loading files from traffic_signs\\3\n",
      "Loading files from traffic_signs\\30\n",
      "Loading files from traffic_signs\\31\n",
      "Loading files from traffic_signs\\32\n",
      "Loading files from traffic_signs\\33\n",
      "Loading files from traffic_signs\\34\n",
      "Loading files from traffic_signs\\35\n",
      "Loading files from traffic_signs\\36\n",
      "Loading files from traffic_signs\\37\n",
      "Loading files from traffic_signs\\38\n",
      "Loading files from traffic_signs\\39\n",
      "Loading files from traffic_signs\\4\n",
      "Loading files from traffic_signs\\40\n",
      "Loading files from traffic_signs\\41\n",
      "Loading files from traffic_signs\\42\n",
      "Loading files from traffic_signs\\5\n",
      "Loading files from traffic_signs\\6\n",
      "Loading files from traffic_signs\\7\n",
      "Loading files from traffic_signs\\8\n",
      "Loading files from traffic_signs\\9\n"
     ]
    }
   ],
   "source": [
    "# getting data\n",
    "images, labels = load_data(data_dir)\n",
    "\n",
    "# converting integer labeles to binary matrices to use categorical_crossentropy loss fn\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# performing train test split\n",
    "x_train, x_test, y_train, y_test =  train_test_split(np.array(images), np.array(labels), test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_19 (Conv2D)          (None, 28, 28, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               1605888   \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 43)                11051     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,617,835\n",
      "Trainable params: 1,617,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 2.3534 - accuracy: 0.3926\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.4848 - accuracy: 0.8920\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2164 - accuracy: 0.9566\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1342 - accuracy: 0.9742\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0907 - accuracy: 0.9835\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0672 - accuracy: 0.9872\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0529 - accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0479 - accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.0477 - accuracy: 0.9894\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0383 - accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2698c49ef70>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "model.fit(x_train, y_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 - 2s - loss: 0.0589 - accuracy: 0.9865 - 2s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05886464565992355, 0.9864864945411682]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to traffic.h5.\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "model.save(filename)\n",
    "print(f\"Model saved to {filename}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
